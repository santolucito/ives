\section{Problem Description}
\label{problem}

\subsection{Problem Formulation}


Program synthesis automatically builds a program from a specification.
Our ultimate goal is to integrate the programming-by-example paradigm 
into mainstream programming environments, thus the generated code must be readable and editable. The generated code should also support a real language and use native data types to the language.

For \ourTool/, the specification is given as a list, $Ex$, of pairs of input and output. The specification is always given in the following 
form:
\begin{lstlisting}[mathescape=true]
exs :: [([$\tau_{in}$], [$\tau_{out}$])]
exs = [$(i_1, o_1), \ldots, (i_n, o_n)$]
\end{lstlisting}


Note that the specification is written in a standard Haskell syntax, enabling this way to make easier the synthesis procedure a part of the language.

We place a constraint on the given example set that $\forall j. 0 \le j \le n. i_j \in \tau_{in} \land o_j \in \tau_{out}$. In practice this a trivial constraint, achieved by requiring the user to provide the types explicitly via type annotation (as we do), or using the compiler to infer types~\cite{ghc}, or by inferring the types based on regular expressions~\cite{gulwani_popl15} .

Let $T$ be the set of types (both inferred and explicitly declared) exposed to the top-level module from user code, imported libraries, and the core library. Additionally, we require that the given specification also satisfies $\tau_{in} \in T \land
\tau_{out} \in T$.

The goal is to find a Haskell expression $e$ such that it holds $\forall j. 0 \le j \le n.\  e (i_j) = o_j$.

This problem is in general undecidable. Finding a function of a type 
$\tau_{in} \rightarrow \tau_{out}$ (or even some of its generalization) is studied in the literature as a type inhabitation problem which is known to be undecidable 
for Haskell. The problem is decidable for 
 the simply typed lambda calculus but is PSPACE-complete~\cite{DBLP:conf/tlca/Urzyczyn97}. 
 As soon as we add polymorphism to the language the type inhabitation problem becomes undecidable~\cite{DBLP:conf/lics/Wells94}. The synthesis problem imposes an additional restriction that even if there is an inhabitant, it also has to satisfy the given examples.

Since the main problem that we are trying to solve is undecidable, we try to find a solution using an incremental algorithm (Sec.~\ref{sec:enumeration}).
 

\subsection{Naturalness}
\label{sec:naturalness}

Our goal is to synthesize \textit{natural} programs, ie. programs that meet a specification with the simplest and most readable code.
As a formalization of simplicity and readability, we propose a definition of naturalness.

Our first idea was to use one of the standard measures for measuring code complexity -- so called {\emph{cyclomatic complexity}}~\cite{McCabe:1976:CM:800253.807712}.
Cyclomatic complexity is function, $\mathcal{CC}$ on the control flow graph of a section of code, $C$.
The function is defined as $\mathcal{CC}(C) = E âˆ’ N + 2P$, where $E$, $N$, and $P$ are the numbers of edges, nodes and connected components respectively in the control flow graph of $C$.
In summary, this function measures the number of linearly independent paths through a program $C$.
However this measure is not well-suited for pure functional languages that use branching in a more functional way.
For example, given two programs to compute absolute value \codeinline{f} and \codeinline{g}, one would expect \codeinline{f} with the "if" solution to be more natural.
\begin{lstlisting}
f x =
  if x>=0 then x else x * (-1)
g x = 
  x * (fromEnum (x>0) + fromEnum (x<0) * (-1))
\end{lstlisting}
Using cyclomatic complexity verbatim means \codeinline{f} is more complex than \codeinline{g}.

Instead, we define naturalness as a function $\mathcal{N}$, that returns the number of nodes in the abstract syntax tree. We consider code $C$ with a lower 
number of $\mathcal{N}(C)$ more natural.

This definition of naturalness matches well many techniques valued in functional community, in particularly 
currying.
In keeping with good functional programming style, 
$\mathcal{N}$(\codeinline{f = (1+)})= 2 is considered more natural than $\mathcal{N}$(\codeinline{f x = 1 + x})= 3.
This also favors predefined functions over generating new lambda terms (anonymous functions).
Taking the code from Listing \ref{natSyn}, $\mathcal{N}$(\codeinline{solution1}) has a naturalness score of 8, while $\mathcal{N}$(\codeinline{solution2}) has a score of 3.

In addition, this definition also integrates basic ideas from cyclomatic complexity, because branching is still encoded as a measure of complexity.

%
We can now formally define the natural synthesis problem: given a set of input-output examples describing user intentions as before, the goal is 
to find an expression $e$, such that it holds
\begin{multline*}
\forall j. 0 \le j \le n.\  e (i_j) = o_j \land \\
 \forall e'. (\forall j. 0 \le j \le n.\  e' (i_j) = o_j) \Rightarrow \mathcal{N}(e) \le \mathcal{N}(e')
\end{multline*}
.




\subsection{Enumeration}
\label{sec:enumeration}
In order to enumerate programs in order of naturalness, we will use a type directed enumeration.
This type directed enumeration will also need to order the terms by their likelihood to satisfy the examples.
Here we describe the space of expressions over which we enumerate, and how this pertains to naturalness.
We describe how the enumeration technique tries to order terms by likelihood to succeed in Sections \ref{synth} and \ref{offline}.
We can immediately restrict our search space to exclude generated lambda terms (anonymous functions), as such terms will generally induce a very low naturalness score and explode the search space.
Our synthesis approach will then only be able to solve synthesis problems when a solution exists that only draws from a finite set of predefined expressions.

Let $E$ be the finite set of expressions exposed to the top-level module from user code, imported libraries, and the core library.
Our search space will be the set of permutations of well-typed applications of elements of $E$.

%To determine if an expression is well-typed, 

A type environment $\Gamma$, is the set $\{e_1 : \tau_1,\ ...,\ e_n : \tau_n\}$, where $e_{i} \in E$ and is of type $\tau_i \in T$.
$\tau_i$ may be a type variable, a concrete type
The set of well-typed expressions we consider, $\mathcal{G}$, is the set $\{e_1 : \tau_1,\ ...,\ e_n : \tau_n\}$, where $e_i : \tau_i$ follows the usual rules of application for constructing well-typed expressions from $\Gamma$.
Notice that a cycle in types of $\Gamma$ will cause $\mathcal{G}$ to be an infinite set.

As an example, consider the following

\begin{gather*}
\Gamma = \{f:a\to b, g:b\to c, x:a\} \Rightarrow \\
\Gamma^2 = \{f:a\to b, g:b\to c, x:a, f(x):b\} \Rightarrow \\
\Gamma^3 = \mathcal{G} = \{f:a\to b, g:b\to c, x:a, f(x):b, g(f(x)):c\}
\end{gather*}

With our definition of $\mathcal{G}$, we now have a decidable space that will allow us to approach the natural synthesis problem.
In fact, we will rely on the definition of this space to ensure we are searching in the neighborhood of high naturalness measure programs.
As our implementation will only be a subset of the space, we do not need to directly encode a measure of naturalness into the algorithm.
Our main task is then to find a efficient solution to the now decidable problem.

\subsection{Lifting Example Types}
While conceptually simple, enumerating all well-typed functions is wasteful.
As an example - if possible we need to prune the search space.
To do this, we can exploit an unstated assumption, but widely accepted approach, in existing programming-by-example work.
Usually, the example pair type is lifted into a function type in the trivial way.

\begin{flalign*}
\lift \ (\tau_i, \tau_o) =\ \tau_i \to \tau_o
\end{flalign*}

However, a subtyping relation can create more specific types that will better prune the space.
The subtyping relation $A<:B$ means that any time type $B$ results in a well-typed program, so would type $A$ in place of $B$.
A subtyping relation induces a subset relation of terms of type $A$ in relation to the terms of type $B$.
For the purposes of this paper, we use $<:$ to mean only nontrivial subtyping rules - that is we do \textit{not} have $A<:A$.
Given $\mathcal{A} = \{ x | x:A\}$ and $\mathcal{B} = \{ x | x:B\}$, we have $\mathcal{A}\subseteq\mathcal{B}$.
Lifting the examples to a subtype of the trivial lifting can then yield a smaller search space.

\begin{flalign*}
\lift'\ (\tau_i, \tau_o) <:\ \tau_i \to \tau_o\\
\end{flalign*}

Notice that we did not write out a full function for the subtype.
This would have implied a subtyping on the component types, specifically the inputs and outputs would be contravariant or covariant, respectively.
However, we do not wish to restrict the domain or range of the function, but only the size of function space.
So we must have the following

\begin{flalign*}
\lift'\ (\tau_i, \tau_o) =&\ \tau^{s}_{i} \to \tau^{s}_{i} \nRightarrow\\
(\tau^{s}_{i} <: \tau_i)\ \lor&\ (\tau_o <: \tau^{s}_{o})\\
\end{flalign*}


As an demonstration of this approach, following the syntax from previous code samples, we demonstrate below the synthesis of \codeinline{map (+1)}. We provide examples of type \codeinline{([Int],[Int])}.
\begin{lstlisting}
exs :: [ [Int] :-> [Int] ]
exs = [
  [1]   :-> [2],
  [3,4] :-> [4,5] ]
\end{lstlisting}

Using the traditional approach, we would have the trivial lifting to the function type.
However, if we use $\lift'$ instead, we would derive a more specific type.
This more specific type could be a refinement type, expressed here using the syntax of LiquidHaskell\cite{DBLP:conf/icfp/VazouSJVJ14}.
 
\begin{lstlisting}
exs        :: ([Int],[Int])
\lift(exs)  :: [Int] -> [Int] 
\lift'(exs) :: x:[Int] -> {y:[Int] |
              length x = length y}
\end{lstlisting}

Alternatively, we could also have derived the equally specific type using dependant types~\cite{DBLP:journals/jfp/McBride02}.
In this case, using the Agda language, we would need a definition of (\codeinline{Vec L a}) to describe the type of lists of length \codeinline{L} and elemental type \codeinline{a}.

\begin{lstlisting}
\lift'(exs) ::
  {Vec L Int -> Vec L Int}
\end{lstlisting}

Notice that in either case, the target function type is a subtype of the trivial lifting, but we have not changed the number of inhabitants of the types of either the domain or range of the target function.

\subsection{Implementation Solution Space}\label{solnSpace}\

Our implementation restricts the search to the first level of application chains.
We are no longer using $\mathcal{G}$ as defined previously, but the more restricted space of $\Gamma^2$.
In the context of the previous example, we stop generating at $\Gamma^2$ and use this as our search space.

\begin{gather*}
\Gamma = \{f:a\to b, g:b\to c, x:a\} \Rightarrow \\
\Gamma^2 = \mathcal{G}' = \{f:a\to b, g:b\to c, x:a, f(x):b\}\\
\end{gather*}

In fact, in implementation we do not need to fully generate $\Gamma^2$.
Since we will not be generating $\Gamma^n, n>2$, we only need to consider functions within $\Gamma^2$, where the input and output types match the examples.

\begin{gather*}
\mathcal{G}_I = \Gamma^2 \cap \{f | f : \tau_i \to \tau_o\}
\end{gather*}

We need this restriction because we are building terms using haskell-src and re-engineering Haskell syntax and application rules, which a very labor intensive approach.
Building longer application chains proved to be out of reach from an engineering perspective.
Luckily, $\Gamma^2$ is able to cover synthesis of a large enough set of programs to be interesting.
However, in order to generate novel and complex programs, we eventually will need to support the proper $\mathcal{G}$ set.
One approach to this would be to use a dependently typed language like Agda~\cite{}.
Rather than reconstructing datatype to manipulate types as value, Agda would allow us to program directly with types.

However, one key component of our approach is the ability to easily support third-party libraries and other user-defined values.
Haskell provides us with a rich set of libraries that allows us to explore this feature of \ourTool/.

While this approach can work for first-order synthesis, we instead focus on data structure manipulation problems that can be solved with higher order functions.

We require all higher order functions to be of a unified type signature $(\star \to \tau_i \to \tau_o)$, so that the signature of the solution program is a function mapping the input type to the output type. 
The type $\star$ is a type wildcard (from the partial type signatures of GHC~\cite{ghc}) that could be filled by a first-order function or a first-order function along with an initial value.
In the case of \codeinline{map :: (a -> b) -> [a] -> [b]}, the $\star$ matches \codeinline{(a -> b)}.
In the case of \codeinline{foldr :: (a -> b -> b) -> b -> [a] -> b)}, the $\star$ matches \codeinline{(a -> b -> b) -> b}.

This is generally a weak constraint, as good functional programming style dictates any higher order function that manipulates a data structure should have such a type signature.
This constraint is used to eliminate higher order functions such as \codeinline{flip:: (a -> b -> c) -> b -> a -> c}.
This function would complicate analysis and management of the search space, and in practice in not often necessary.

%\markk{the paragraph below makes no sense to me at all. better leave it out}

%The user must partially uncurry (collapsing trailing function arguments into a single tuple argument) any higher-order function they are interested in using during synthesis.
%This also means that any type variable appearing in the higher-order function must be accounted for in the input and output types so that all type variables in its signature can be resolved.
%This allows us to conclude that any types that are between the input and first order function will be static initial values, which can be assigned using the process described in Section \ref{makeFxns}.
%This is a simple procedure that makes use of the user's domain knowledge of which parameters to the function will be given by the examples; consider:

%\begin{lstlisting}
%zipWith' :: (a -> b -> c) -> ([a], [b]) -> [c]
%zipWith' f (xs,ys) = zipWith f xs ys
%\end{lstlisting}

Generally, the component function is applied across the \textsf{input} data structure, which the \textsf{solution} uses to construct an \textsf{output} data structure or reduction.
As we will demonstrate in Section \ref{evaluation} this set is expressive enough to support the classic \texttt{map}, \texttt{filter}, and \texttt{fold} functions, as well as higher order functions found in imported modules and user-supplied code.

The approach presented here to efficiently solve natural synthesis is then: given a type environment $\Gamma$ and an example set $Ex:\{(\tau_i,\tau_o)\}$, enumerate $\mathcal{G}_I$ to efficiently approximate naturalness.
This list can then be checked in order to find an $e$ such that $\forall (i,o) \in Ex, e (i) = o$.
