\section{Problem Formulation}
\label{problem}

\subsection{Naturalness}

The goal in natural synthesis is synthesize programs that meet a specification with the simplest and most readable code.
For programming by example, the specification is a set, $Ex$, of pairs of input and output $(i,o)$.
As a formalization of simplicity and readability, we present a definition of naturalness.

The standard of measuring complexity (the inverse of naturalness) is cyclomatic complexity\cite{cyclo}.
Cyclomatic complexity is function, $\mathcal{CC}$on the control flow graph of a section of code, $C$.
The function is $\mathcal{CC}(C) = E âˆ’ N + 2(P)$, where $E$, $N$, and $P$ are the edges, nodes and connected components respectively.
This function measures "the number of linearly independent paths" through a program, a crucial part of manipulating stateful variables.
However a this is not well-suited for pure functional languages that use branching in a more functional way.
For example, given two programs to compute absolute value, the "if" solution should be more natural, but using cycolmatic complexity verbatim means
\codeinline{f x = if x>=0 then x else x*-1} is more complex
\codeinline{f x = x * (fromEnum (x<0) * (-1) + fromEnum (x>0))}

Instead, we define naturalness to be the number of nodes in the abstract syntax tree.
This approach integrates cyclomatic complexity, because branching is still encoded as a measure of complexity.

%Our definition of naturalness matches well many techniques valued in functional community.
Currying fits under this definition of naturalness.
\codeinline{f = (1+)} is more natural than \codeinline{f x= 1 + x}.
This also favors predefined functions over generating new lambda terms.
Taking the code from Listing \ref{natSyn}, $\mathcal{N}($solution1$)$ has a naturalness score of 1/8, while $\mathcal{N}($solution2$)$ has a score of 1/3.

The problem is then to find an expression e, such that $\forall (i,o) \in Ex, e (i) = o \land max(\mathcal{N}(e))$.


\subsection{Enumeration}
In order to solve the above problem, we will immediately restrict our search space to exclude generated lambda terms, as such terms will generally induce a very low naturalness score and explode the search space.
Our synthesis approach will then only be able to solve synthesis problems when a solution exists that only draws from a finite set of predefined expressions.

Let $E$ be the finite set of expressions exposed to the top-level module from user code, imported libraries, and the core library.
Our search space will be the set of permutations of well-typed applications of elements of $E$.

To determine if an expression is well-typed, let $T$ be the set of types (both inferred and explicitly declared) exposed to the top-level module from user code, imported libraries, and the core library.
A type environment $\Gamma$, is the set $\{e1 : \tau_1,\ ...,\ e_n : \tau_n\}$, where $e_{i} \in E$ and is of type $\tau_i \in T$.
The set of well-typed expressions we consider, $\mathcal{G}$, is the infinite set $\{e1 : \tau_1,\ ...,\ e_n : \tau_n\}$, where $e_i : \tau_i$ follows the usual rules of application for constructing well-typed expressions from $\Gamma$. 

We place a constraint on the example set that $Ex:\{(\tau_i,\tau_o)\}$, or more specifically $\forall (i,o) \in Ex,\ i:(\tau_i \in T) \land o:(\tau_o \in T)$.
In practice this a trivial constraint, achieved by requiring the user to provide the types explicitly \cite{Osera:2015} or to infer the types \cite{gulwani_popl15} based on regular expressions.



\subsection{Lifting Example Types}
While conceptually simple, enumerating all well-typed functions is wasteful - if possible we need to prune the search space.
To do this, we can exploit an unstated assumption, but widely accepted approach, in existing programming-by-example work.
Usually, the example pair type is lifted into a function type in the trivial way.

\begin{flalign*}
lift\ (\tau_i, \tau_o) =\ \tau_i \to \tau_o
\end{flalign*}

However, a subtyping relation can create more specific types that will better prune the space.
The subtyping relation $A<:B$ means that any time type $B$ results in a well-typed program, so would type $A$ in place of $B$.
A subtyping relation induces a subset relation of terms of type $A$ in relation to the terms of type $B$.
Given $\mathcal{A} = \{ x | x::A\}$ and $\mathcal{B} = \{ x | x::B\}$, we have $\mathcal{A}\subseteq\mathcal{B}$.
Lifting the examples to a subtype of the trivial lifting can then yield a smaller search space.

\begin{flalign*}
lift'\ (\tau_i, \tau_o) <:\ \tau_i \to \tau_o\\
\end{flalign*}

Notice that we did not write out a full function for the subtype.
This would have implied a subtyping on the component types, specifically the inputs and outputs would be contravariant or covariant, respectively.
However, we do not wish to restrict the domain or range of the function, but only the size of function space.
So we must have the following

\begin{flalign*}
lift'\ (\tau_i, \tau_o) =&\ \tau^{s}_{i} \to \tau^{s}_{i} \nRightarrow\\
(\tau^{s}_{i} <: \tau_i)\ \lor&\ (\tau_o <: \tau^{s}_{o})
\end{flalign*}


As an demonstration of this approach, following the syntax from previous code samples, we demonstrate below the synthesis of \codeinline{map (+1)}. We provide examples of type \codeinline{([Int],[Int])}.
\begin{lstlisting}
exs :: [ [Int] :-> [Int] ]
exs = [
  [1]   :-> [2],
  [3,4] :-> [4,5] ]
\end{lstlisting}

Using the traditional approach, we would have the trivial lifting to the function type.
However, if we use $lift'$ instead, we would derive a more specific type.
This more specific type could be a refinement type, expressed here using the syntax of LiquidHaskell\cite{DBLP:conf/icfp/VazouSJVJ14}.
 
\begin{lstlisting}
exs :: [Int],[Int]
lift(exs) :: [Int] -> [Int] 
lift'(exs) ::
  {x:[Int] -> y:[Int] | length x > length y}
\end{lstlisting}

Alternatively, we could also have derived the equally specific type using dependant types\cite{dependant_types}.
In this case, we would need a definition of (\codeinline{Vec L a}) to describe the type of lists of length \codeinline{L} and elemental type \codeinline{a}.

\begin{lstlisting}
lift'(exs) ::
  {Vec L Int -> Vec L Int}
\end{lstlisting}


Notice that in either case, the target function type is a subtype of the trivial lifting, but we have not changed either the types of either the domain or range of the target function.

Our approach to natural synthesis is then: given a type environment $\Gamma$ and an example set $Ex:\{(\tau_i,\tau_o)\}$, enumerate $\mathcal{G}$ in order of naturalness, such that $e : lift'(\tau_i,\tau_o)$.
This list can then be checked in order to find an $e$ such that $\forall (i,o) \in Ex, e (i) = o$.

\subsection{function classification}
we need to assign all functions in $\mathcal{G}$ a refinement type in order to use the above result.
Hence we do offline analysis.



%
%This framing will draw the type of the target expression directly from the examples.

\subsection{Solution Space}\label{solnSpace}
While this approach can work for first-order synthesis, we instead focus on data structure manipulation problems that can be solved with higher order functions.

We require all higher order functions to be of a unified signature \texttt{$\_ \to * \to *$}, where the final kind of the signature is a function mapping the input type to the output type. 
Here, a kind is understood to be the type of a type constructor, in this case \texttt{$\to$}, which constructs a function type from two other types.

The practical consequence of this format is that a user must partially uncurry (collapsing trailing function arguments into a single tuple argument) any higher-order function they are interested in using during synthesis.
This also means that any type variable appearing in the higher-order function must be accounted for in the input and output types so that all type variables in its signature can be resolved.
This allows us to conclude that any types that are between the input and first order function will be static initial values, which can be assigned using the process described in Section \ref{makeFxns}.
This is a simple procedure that makes use of the user's domain knowledge of which parameters to the function will be given by the examples; consider:

\begin{lstlisting}
zipWith' :: (a -> b -> c) -> ([a], [b]) -> [c]
zipWith' f (xs,ys) = zipWith f xs ys
\end{lstlisting}

By formally defining the space of functions we are interested in synthesizing, we can this definition to prove some properties on the algorithm.
In particular we show in Section \ref{sound} that \ourTool/ is complete for this subset of functions.

the solutions \ourTool/ supports synthesizing are higher-order data structure manipulation programs.
The higher-order functions take a component function that is a first-order function, for example \codeinline{(+)}.
The solution programs can be expressed as:
% up to reordering of terms (we dont actually support this, should we really include this)

\begin{lstlisting}
solution ::
           (* -> types)  -- Component Function
        ->  types        -- Initial Values
        ->  *            -- Input
        ->  *            -- Output
types = * | * -> types
-- * matches on type variables and constructors.
\end{lstlisting}

Generally, the component function is applied across the \textsf{input} data structure, which the \textsf{solution} uses to construct an \textsf{output} data structure or reduction. As we will argue in Section \ref{evaluation} this set is expressive enough to support the classic \texttt{map}, \texttt{filter}, and \texttt{fold} functions, as well as higher order functions found in imported modules and user-supplied code.

Our goal is to create a synthesis procedure that is easily portable across full implementations of functional languages (Haskell, OCaml, etc), so we prefer using a type directed approach to synthesis over explicit code analysis whenever possible. This increases the portability and longevity of our system. For this implementation we target Haskell, detailing the exact modifications needed to expand this to other languages in Section \ref{languageSupport}.

%Our algorithm does not explicitly try to fit component functions to the examples. Instead, we leverage a promising body of existing work in synthesizing top-level, first-order functions \cite{potential, reviewers}. While it is out of scope to go in to detail, we will briefly discuss the integration of these synthesis procedures in Section \ref{conclusions}.

%The liquidHaskell predicate applied to this signature will be of the effect of \texttt{len([a],[b]) = len([c])}.
