\section{Related Work}
\label{sec:related}

In describing \ourTool/, we have introduced the problem of natural synthesis. 
We have shown how to formulate an enumerative type-directed approach synthesis that results in natural code, but also frees us from burdensome constraints of code analysis and hard coded inference rules.
Many of the techniques we have used have been explored in various contexts before, though generally for the purpose of lower level synthesis. 
In this section we make some comparisons to related work, and highlight the differences we employ that help us generate readable code.

One of the most closely related works in aspirations is a tool called MagicHaskeller~\cite{DBLP:conf/aaip/Katayama09}. This project makes heavy use of a ranking system based on code use and lookup frequency in a database to deliver natural results to the user. In contrast to our work, MagicHaskeller uses a database of functions as its main synthesis engine, with the current database hovering around
64GB~\cite{DBLP:conf/agi/Katayama15}. From this work, we take the inspiration of supporting imported libraries for creating natural code. However, it is important that the system is more portable and easily manipulated by the user - in particular by allowing user defined function in synthesis.

MagicHaskeller works in the same AI focused domain of inductive programming as the tool IgorII~\cite{DBLP:conf/aaip/HofmannKS09}. IgorII however takes a very code analysis heavy approach, having been originally developed for Maude, then ported to Haskell. Although these approaches tend to generate readable code, there has not yet been formal analysis of this feature.

One of the motivating works for exploring type-directed programming by example, especially over recursively defined datatypes, is MYTH~\cite{Osera:2015, Osera:2016}. The natural extension of this work in the usability direction was to include a more lightweight and flexible support for user defined and imported datatypes. The $\Lambda^2$ tool also focuses on deriving programs over recursively defined datatypes~\cite{Feser:2015}. One of the major barriers to an average user with these tools, is that the generated code operates on the inner workings on a datatype. While this provides a complete picture of all the data manipulation, often a user might prefer to simply be provided with high level, functioning code. Building in support and the ability to reason on user defined functions in \ourTool/ has made this natural synthesis possible.

\cite{Osera:2016} ``does not infer instantiation of polymorphic library functions, and the theoretical framework restricts the combination of union and intersection types''.
This limitation is due to the ``programs as proofs'' approach to synthesis.
That approach is more complex than enumeration, and unsurprisingly faster, but is limited by a heavy theoretical framework.
However, the approach in ~\cite{Osera:2016} can solve many synthesis problem because of the inclusion of lambda terms in the search space.

Another valuable contribution from~\cite{Osera:2016} is the relationship between refinement types and examples.
This allows the system to use refinement types to extend the specification language of a programming by example system.
Instead, \ourTool/ keeps refinement types entirely as a backend logical inference technique and hidden from the user.
In line with our goal of synthesizing natural code, we wish to minimize  asking the onerous task of users to learn to write new specifications.
Simple as they may be, refinement types are not yet seem as approachable as the more familiar ``examples as a specification'' to the average user.

Taking the refinement types as a specification even further,~\cite{dblp1683325} proposes a system Synquid, that will synthesize programs based on refinement types. 
At first glance this seems to be an entirely different approach than \ourTool/, which instead uses examples as the specification and only uses refinement types as a search space pruning tool.
However, the work in~\cite{Osera:2016} does give an indication that our use of refinement types may be related on fundamental, proof theoretic level. 
By exploring this relationship in more depth, it may be possible to draw a stronger parallel between these various works and port ideas from one system to another.

One of the most widely used and well known instances of a programming by example system being used by many novice users is FlashFill~\cite{GulwaniHS12}.
Like other system, the goal of this work is to make executable, not code. 
This leaves users without the ability to modify generated source code.
In the case of FlashFill, being embedding within production level software, most users are not clamoring for this feature.
However it would certainly open an interesting avenue to introduce new users to computer programming if this were an option.
StriSynth~\cite{icse} takes a step in this direction by providing a natural language description of the synthesized program, but the source code remains obfuscated and inaccessible.

One difficult limitation is that without subexample generation we cannot recursively apply our algorithm as in the $\Lambda^2$ tool~\cite{Feser:2015}.
Subexample generation gives the ability to recursively call the synthesis engine to generate programs with multiple applications of high order functions.
However, since the ability of $\Lambda^2$ to generate subexamples relied on hard coded subexample generation hypotheses for the predefined set of higher order functions, this does not scale.
While inferring the hypotheses might be possible by inspecting the code, we have maintained a dedication to minimize our reliance on code analysis techniques for portability and longevity of the system.
The best way to automatically create subexample generation functions solely based on type information remains a difficult problem.






